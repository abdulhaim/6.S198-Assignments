<!DOCTYPE html>
<html>
	<head>
		<title>Assignment 2</title>
		<link rel="stylesheet" type="text/css" href="css/main.css">
	</head>
	<body>
		<h1> Assignment 2 </h1>
		<h3> <i> Due Wednesday September 12th at 10 AM </i> </h3>

		<h2> <b> 1.1: Building models with model builder (In-class Monday) </b> </h2>

		   <p style = "font-size:18px ; line-height: 1.4em; color: #333 "> 

		   	<img src="images/assignment_2/invalid_model.png" alt="Teachable Machine" align="middle" height="200" width="400" />	

		   	<br/> <br/>

		    On the leftmost column of Model Builder, the initial model is marked Invalid Model. This model is invalid as the output from the Input Image layer ([28,28,1]) does not match with the input of the Softmax Cross-Entropy layer layer ([10]).

		   <br/> <br/>

		   <img src="images/assignment_2/wrong_classification.png" alt="Teachable Machine" align="middle" height="200" width="400" />

		   <br/> <br/>

		   Problem 1: The classifications above that we are seeing are almost always wrong. This is because the parameters as part of the equation <i> Wx + b </i> are randomly initialized and not trained further to find optimal values. Hence we see that the network has a classfication rate of approx 10-15%, which is what would be expect as it equivalent to choosing a label at random from the 10 options avaliable. 

		   <br/> <br/>
 
			</p>

			<h2> <b> 1.2: Training </b> </h2>

			<p style = "font-size:18px ; line-height: 1.4em; color: #333 "> 

			Problem 2: 

			1. Here are the observations made for training MNIST, CIFAR-10, and Fashion MNIST respectively:

			<br/> <br/>

			<img src="images/assignment_2/mnist_simple.png" alt="Teachable Machine" align="middle" height="200" width="400" />	

			<br/> <br/>

			MNIST: Training accuracy ranges from 80%-92% with the demo making approximately 1170 inferences per second. The model trains 1230 examples/second as shown in the visuals above.

			<br/> <br/>

			<img src="images/assignment_2/fashion_mnist_simple.png" alt="Teachable Machine" align="middle" height="200" width="400" />

			<br/> <br/>

			Fashion MNIST: Training accuracy ranges from 73%-76% with the demo making approximately 1250 inferences per second. The model trains 1240 examples/second as shown in the visuals above.

			<br/> <br/>

			<img src="images/assignment_2/cifar_simple.png" alt="Teachable Machine" align="middle" height="200" width="400" />	

		    <br/> <br/>

			2. After changing the dataset to CIFAR-10, I observed a training accuracy ranging from 32%-45% with the demo making approximately 705 inferences per second. The model trains 1030 examples/second as shown in the visuals above.

			<br/> <br/>

			3. The Fully Connected Unit layer Added is shown here:
		   	
		   	<br/> <br/>

			<img src="images/assignment_2/mnist_fully_connected.png" alt="Teachable Machine" align="middle" height="200" width="400" />	

		   	<br/> <br/>

			4. Changing back to MNIST, we try to improve the accuracy by adding more fully connected units, one on top of the other as such: Input → Flatten → FC(10) → FC(10) → Softmax → Label

			The accuracy goes to zero and the network performs terribly. The inference probability also becomes Nan% for the images. This is due to having two Fully Connected Layers one after the other in the network, which causes the network to diverge and leads to exploding gradients. In order to correct this, we should add a non-linear layer such as RELU instead of putting two linear layers one after the other. 

			</p>

		<h2> <b> 1.4: Activation Layers </b> </h2>

			<p style = "font-size:18px ; line-height: 1.4em; color: #333 "> 

			Problem 3: 

			Instead of adding two Fully Connected Layers one after the other, we add a ReLU layer between the FC layers to make:

		   <br/> <br/>

			Input → Flatten → FC(10) → ReLU → FC(10) → Softmax → Label

		   <br/> <br/>

			After the training the new model, we see a training accuracy 62%-78% of and a testing accuracy 57%-76% as shown below. There are no more Nan values as well. You can see further train statistics below:

		   <br/> <br/>

			<img src="images/assignment_2/add_relu.png" alt="Teachable Machine" align="middle" height="200" width="400" />	

			<br/> <br/>

			However after increasing the number of units of the first FC model from 10 to 100, we see a noticable difference as seen below. The training and test accuracy is approximately 93%-98%, much higher than previously. This can be explained by 

			<br/> <br/>

			<img src="images/assignment_2/change_fc_units.png" alt="Teachable Machine" align="middle" height="200" width="400" />

			</p>

			<h2> <b> 1.6: Exploring with Model Builder </b> </h2>

			<p style = "font-size:18px ; line-height: 1.4em; color: #333 "> 

			Problem 4: 
			
			<br/> <br/>

			1. After training the MNIST model with 1,2,3,4 and 5 FC layers (and ReLU between them and the same hyperparameters and number of hidden units), I found a training time of blank and an acccuracy time of blank as shown below.

			<br/> <br/>

			The model seems to overfit for What were the training times and accuracy? Do you see any overfitting? What can you conclude about how many layers to use? Include screenshots of the Training Stats for each of your examples.

			</p>

			<h2> <b> 2.1: Setting Up to code Multilayer Models</b> </h2>

			<p style = "font-size:18px ; line-height: 1.4em; color: #333 "> 

			Problem 5: 

			As you increase the NUM_BATCHES from 50 to 300, the loss converges to a lower value. Increasing the BATCH_SIZE does not cause any effect in the loss but causes less variation in the loss. In general, I see that as you increase on or the other the training time increases.

			</p>

			<h2> <b> 2.2: Training and Testing </b> </h2>

			<p style = "font-size:18px ; line-height: 1.4em; color: #333 "> 


			TEST_ITERATION_FREQUENCY determines the number of batches the model trains on between tests. TEST_BATCH_SIZE determines how many images the test is run on each time. 

			<br/> <br/>

			Problem 6

			1. Examples of classifications where the system does poorly/is wrong include when the numbers are bolded/have a thick font size or when they are not proportionate.

			<img src="images/assignment_2/misclassify.png" alt="Teachable Machine" align="middle" height="200" width="400" />

 			<br/> <br/>

			2. I experimented with changing the batch size and the number of batches to try to improve the testing results. Overall, I observed that increasing the batch size and the number of batches increases the last accuracy value. However, increasing both increases the time in which it takes to train the model, so I tried to find combinations that would bring similar results.

			<br/> <br/>


			<img src="images/assignment_2/300_200.png" alt="Teachable Machine" align="middle" height="200" width="400" />

			<br/> <br/>


			Below you can see the highly last accuracy value of 82% with a batch size of 300 and a number of batches value of 200. This accuracy value is pretty consistent for high values of these two variables. However when the batch size is 30 and the number of batches value is 100, we get an accuracy value of 80% although this is not a very consistent result. 

		   <br/> <br/>

			I also tried a batch size of 30 and a high number of batches value of 200 and got an accuracy result of 96.67%.  

			<br/> <br/>


			3. To add a ReLU layer after a dense layer, use the dense layer’s activation parameter, rather than explicitly adding a separate layer, as the following: 

			model.add(tf.layers.dense({
			  units: 10,
			  activation: 'relu', 
			  kernelInitializer: 'varianceScaling'
			}));

			<br/> <br/>


			<img src="images/assignment_2/layers.png" alt="Teachable Machine" align="middle" height="600" width="400" />

			<br/> <br/>


			I made the model more complex by adding 2 more additional layers (3 Fully Connected layers of 100 units each, with RELU layers in between). I got an accuracy of 82% with a batch size value of 100 and a number of batches value of 300.

			<br/> <br/>

			<img src="images/assignment_2/mnist_many_layers_performance.png" alt="Teachable Machine" align="middle" height="200" width="400" />

			<br/> <br/>

			After exploring with MNIST, I tried the same network with the Fashion MNIST data (with higher values for the two parameters of 300 for batch size and 400 for the number of batches respectively) as shown below: 

			<br/> <br/>

			<img src="images/assignment_2/fashion_mnist.png" alt="Teachable Machine" align="middle" height="200" width="400" />

			<br/> <br/>

			</p>

			<h2> <b> 2.3: Style Transfer Example </b> </h2>

			<p style = "font-size:18px ; line-height: 1.4em; color: #333 "> 

			Style transfer is an application of convolutional neural networks. Given a content image and a style (another image), we can apply a computation that transforms the content to match the selected style.

			Here are some of the examples I created on DeepArt:

			<br/> <br/>

			<img src="images/assignment_2/deep_art_1.png" alt="Teachable Machine" align="middle" height="200" width="600" />

			<br/> <br/>

			<img src="images/assignment_2/deep_art_2.png" alt="Teachable Machine" align="middle" height="200" width="600" />





			</p>


		</body>



	</body>

</html>





