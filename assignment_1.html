<!DOCTYPE html>
<html>
	<head>
		<title>Assignment 1</title>
		<link rel="stylesheet" type="text/css" href="css/main.css">
	</head>
	<body>
		<h1> Assignment 1 </h1>
		<h3> <i> Due Monday September 10th at 10 AM </i> </h3>

		<h3> <b> Build Teachable Machine on Your Computer </b> </h3>

		<img src="images/assignment_1/teachable_machine.png" alt="Teachable Machine" align="middle" height="200" width="400" />

		<h2> <b> Train Some Classes </b> </h2>

		   <p style = "font-size:18px ; line-height: 1.4em; color: #333 "> For this assignment, we were instructed to run some experiments with a partner and note down our observations. 

			   <br/> <br/> <u><i>Task 1:</i></u> We trained the network with a series of our facial expressions and observed that the network can tell the difference between the two faces with a confidence of greater than 95%. 
				
			   <br/> <br/><img src="images/assignment_1/multiple_objects.png" alt="Multiple Objects" align="middle" height="200" width="400" />

			   <br/> <br/> <u><i>Task 2:</u></i> I trained the network on various inanimate objects, including my phone, notebook, and water bottle and observed a confidence of minimum 95%. However as I take the object further away from the camera, it begins to misclassify. For example, the bottle is mistaken to be the phone with a confidence level of approximately 75% due to the similarity of color between the two objects. 

			   <br/> <br/><img src="images/assignment_1/fixed_expression.png" alt="Fixed Expression" align="middle" height="200" width="400" />

			   <br/> <br/>  <u> <i>Task 3:</u></i> I trained the network on two different classes with the same image set; specifically my two classes were the same (my face in a fixed facial expression). In this case, I observed the confidence shift between the two classes with each having approx 50%.

			   <br/> <br/> <u><i>Task 4:</u></i> With the first class defined as two different people having Expression A and the second class defined as two people having Expression B, the classifier was able to distinguish between the two facial expressions at a confidence of 90%. In my case, my first class was a smiling facial expression and my second class was a confused facial expression.

			   <br/><br/> <img src="images/assignment_1/distance.png" alt="Fixed Expression" align="middle" height="200" width="400" />

			   <br/><br/><img src="images/assignment_1/class_3.png" alt="Fixed Expression" align="middle" height="200" width="400" />
			   
			   <br/><br/>  <u> <i>Task 5:</u></i> With the first class defined as person A very close to the camera and the second class defined as a person B very far, the network was very confused and would sometimes misclassify person A as person B and it could be argued that it was training based on distance. I also trained the network with a 3rd class of person B close to the camera, observing that the network could now identify faces with a confidence of 95%/

			   <br/> <br/> <u><i>Task 6:</u></i> <a href="http://lesswrong.com/lw/7qz/machine_learning_and_unintended_consequences/"> The Russian Tank Parable </a> is used to illustrate when a network learns based on a different feature in the image rather than the one you assume - in this scenario, the images were trained on the classes cloudy/sunny rather than trees/camouflaged tanks. 
			   <br/> Some examples to illustrate this include: 
			   <br/> <br/> Example 1: The above question demonstrates the Russian Tank Parable as the network trained based on close verus far rather than on the face of the person. 

			   	<br/> <br/> Example 2: First class defined as a pencil, Second Class defined as a pen. The intended classification is the object but the real classification could be the background of the object (kitchen versus classroom)

			</p>


		<h2> <b> How Teachable Machine Works </b> </h2>

			<p style = "font-size:18px ; line-height: 1.4em; color: #333 "> Teachable Machine is based on representing images as points in a high-dimensional vector space. The vector space coordinates are generated by a multilayer convolutional neural network called squeezenet. The version of squeezenet included in Teachable Machine has been trained on millions of images. Given a new image, it produces 1000 values that reflect various attributes of the images, presented as a 1000-dimensional "logits" vector. 

			<br/> <br/>  I did a little more research on squeezenet and found these links helpful: https://www.kdnuggets.com/2016/09/deep-learning-reading-group-squeezenet.html 

			</p>

		<h2> <b> Examining the Confidence Levels </b> </h2>

		<br/><img src="images/assignment_1/dialog.png" alt="Fixed Expression" align="middle" height="200" width="400" /> <br/> <br/>

		<p style = "font-size:18px ; line-height: 1.4em; color: #333 "> 

		 	 <u> <i>Task 1:</u></i> I stored the variables <i> confidences</i> and <i>nCounts</i> in the global variables 
		 	 <i>globConf</i> and <i>globNCounts</i> respectively and used the window.alert function to display a dialog with the number of closest matches and the confidence level. <br/><br/>

			 <u><i>Task 2:</u></i> After redoing the experiments from section 2, I observed the following (with support from confidences and top K values):

			<br/><br/>

			Experiment 1: Inanimate Objects - I trained the network on the same inanimate object as classes (bottle, notebook, and iPad) with the same background, and observed that all the top K images were from the a single class with a confidence of almost 100% for test images that were the same as the trained images. 

			<br/><br/>

			Experiment 2: Inanimate Objects with Distance - The same was observed for the bottle object placed very far away from the screen. However for images of notebook and iPad, the network was very confused as these objects are of the same shape but different color. 

			<br/><br/>

			Experiment 3: Different Background, Same Object - With the same 3 classes as before, I trained the network with an object on one background and tested it on another background. I saw that the network had trained on the background and was unable to classify the object to its correct class.

		</p>

		<h2> <b> Scaling the Confidence Values </b> </h2>

		<p style = "font-size:18px ; line-height: 1.4em; color: #333 "> 

		<u><i>Task 1:</i></u> The method computeConfidences() originally computes confidence values for each class by adding up the number of topK images in each class (nCounts) and dividing by K (kVal = 20 maximum). 
		<br/><br/>

		I explored two alternative ways of computing the confidence.

		<br/><br/>

		A. Scaling the Confidences (Weighted Approach):

		I calculate the weighted confidence for each class as follows: <br/>

		confidence for class X = (number of topK images for class X) / ((number of total images in class X) * (kVal))

		<br/><br/>

		B. Threshold for Confidence 
		For this approach, the confidence for a class is greater than zero only if the number of topK images is greater than 1. For all classes that satisfy this condition, the total confidence is divided equally. <br/>

		confidence for class X = (number of topK images for class X)/kVal

		<br/><br/>

		<u><i>Task 2:</i></u> With these modified confidence calculations, I tried some of the scenarios that confused the network previously, including detecting an object in different backgrounds as well as detecting different objects at different distances away from the camera. 

		Method A: The network performs better for this method in situations where there is a difference between the number of samples that were used for training between the two samples, since we are now scaling the confidences.

		<br/><br/>

		For example, for 3 classes (Notebook, Bottle, iPhone) each with 122, 30, and 60 images, the confidence values are as follows: 

		<br/><br/>
		Modified Results: [0.4737, 0.2632, 0.2632]

		<br/><br/>
		Unmodified Results: [0.4737, 0.2632, 0.2632]

		<br/><br/>

		Method B: The network performs better for this method in situations where there is a lot of overlap between the images for two different objects due to background. It also performs better for objects that look very similar in shape (such as the notebook verus iPad scenario).

		For example, for 3 classes (Notebook, Bottle, iPhone) each with 122, 30, and 60 images, the confidence values are as follows: 

		<br/><br/>
		Modified Results: [0.0455, 0.0455, 0.9091]

		<br/><br/>
		Unmodified Results: [0.25, 0.25, 0.5] 

		<br/><br/>
		<u><i>Task 3:</i></u> Some image classification situations where the above alternative ways of confidences may be useful include cases where it is important to weigh classes that have a better match as there may be noise in the data due to the background or a small number of images used to train for a specific class.

		<br/> <br/> 

		Modified code for the WebcamClassifier can be found <a href="WebcamClassifier.js">here </a> 

		</p>

		<h2> <b> Limiting the number of training examples per class </b> </h2>

		<u><i>Task 1:</i></u> I changed the animate() method such that each class has an allocated number of samples it can accept. An alert window pops up notifying the user that no more samples can be accepted as shown in the image below. 


		<br/><img src="images/assignment_1/maximum_images.png" alt="Maximum Images" align="middle" height="200" width="400" /> <br/> <br/>



		2. Once youâ€™ve finished the  implementation, explore what happens if the number of training images for the classes is severely unbalanced. Try using both easy-to-classify cases and some of the cases that confused your system previously. 

		<h2> <b> Further Explorations </b> </h2>
		<h2> <b> Readings I Found Useful </b> </h2>

		</p>

	</body>

</html>





